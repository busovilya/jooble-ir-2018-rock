{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import requests\n",
    "import configparser\n",
    "\n",
    "from json import JSONDecodeError\n",
    "from collections import Counter\n",
    "from flask import Flask\n",
    "from flask import request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"use this function for creating inverted index\"\n",
    "start = time.time()\n",
    "inverted_index = create_inverted_index()\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"create idf and normilized tf_idf from saved inverted_index\"\n",
    "dict_idf = idf(inverted_index)\n",
    "dict_tf_idf = normalized_tf_idf_docs(inverted_index, docs, dict_idf)\n",
    "requests.post('http://127.0.0.1:13541/rank/idf', json={'idf' : dict_idf,\n",
    "                                                      'tf_idf': dict_tf_idf})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"save inverted index for using at home\"\n",
    "with open('reverse_index.pickle', 'wb') as handle:\n",
    "    pickle.dump(inverted_index, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"you can use saved inverted index at home\"\n",
    "with open('reverse_index.pickle', 'rb') as file:\n",
    "    inverted_index = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_text = []\n",
    "for root, dirs, files in os.walk(\"../../Data/by\"):  \n",
    "    for filename in files:\n",
    "        if 'text' in filename and '-01-' in filename:\n",
    "            with gzip.open('../../Data/by/' + filename, 'rb') as f:\n",
    "                for line in f:\n",
    "                    try: \n",
    "                        docs_text.append(json.loads(line))\n",
    "                    except JSONDecodeError:\n",
    "                        print(\"Can't read file \" + filename)\n",
    "\n",
    "meta_docs = []\n",
    "for root, dirs, files in os.walk(\"../../Data/by\"):  \n",
    "    for filename in files:\n",
    "        if 'text' not in filename and '-01-' in filename:\n",
    "            with gzip.open('../../Data/by/' + filename, 'rb') as f:\n",
    "                for line in f:\n",
    "                    try: \n",
    "                        meta_docs.append(json.loads(line))\n",
    "                    except JSONDecodeError:\n",
    "                        print(\"Can't read file \" + filename)\n",
    "\n",
    "meta_docs = pd.DataFrame(meta_docs)\n",
    "meta_docs.drop_duplicates(subset='id', keep='first', inplace=True)\n",
    "docs_text = pd.DataFrame(docs_text)\n",
    "docs_text.drop_duplicates(subset='id_job', keep='first', inplace=True)\n",
    "docs=pd.merge(meta_docs, docs_text, how='inner', left_on='id', right_on='id_job')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_docs(path, files):\n",
    "    meta_docs = []\n",
    "    docs_text = []\n",
    "    for file_name in files:\n",
    "        if 'text' not in file_name:\n",
    "            with gzip.open(path + file_name, 'rb') as f:\n",
    "                for line in f:\n",
    "                    try: \n",
    "                        meta_docs.append(json.loads(line))\n",
    "                    except JSONDecodeError:\n",
    "                        print(\"Can't read file \" + filename)\n",
    "        \n",
    "        if 'text' in file_name:\n",
    "            with gzip.open(path + file_name, 'rb') as f:\n",
    "                for line in f:\n",
    "                    try: \n",
    "                        docs_text.append(json.loads(line))\n",
    "                    except JSONDecodeError:\n",
    "                        print(\"Can't read file \" + filename)\n",
    "                    \n",
    "    meta_docs = pd.DataFrame(meta_docs)\n",
    "    meta_docs.drop_duplicates(subset='id', keep='first', inplace=True)\n",
    "    docs_text = pd.DataFrame(docs_text)\n",
    "    docs_text.drop_duplicates(subset='id_job', keep='first', inplace=True)\n",
    "    docs=pd.merge(meta_docs, docs_text, how='inner', left_on='id', right_on='id_job')\n",
    "    \n",
    "    return docs      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def position_in_text(lst, term):\n",
    "    '''\n",
    "    lst - list of tokens\n",
    "    term - term that you are looking for\n",
    "    Return all positions of term in the list of tokens(0 = first token) \n",
    "    '''\n",
    "    return [i for i, x in enumerate(lst) if x == term]\n",
    "\n",
    "def position_in_sentences(lst, term):\n",
    "    '''\n",
    "    lst - list of list, where each nested list - seperate sentences\n",
    "    term - term that you are looking for\n",
    "    Return all sequence numbers of sentences in which you can find term (0 = first sentence) \n",
    "    '''    \n",
    "    return [i for i, x in enumerate(lst) if term in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idf(inverted_index):\n",
    "    '''\n",
    "    The function returns inverted document frequency\n",
    "    for each term in inverted_index.\n",
    "    \n",
    "    Input: inverted_index - fresh inverted index.\n",
    "    '''\n",
    "    number_of_docs = docs.shape[0]\n",
    "    idf = {term: np.log(number_of_docs / len(inverted_index[term])) for term in list(inverted_index.keys())}    \n",
    "    return idf\n",
    "\n",
    "def tf(document):\n",
    "    '''\n",
    "    The function returns tf for a certain document\n",
    "    \n",
    "    Input: dictionary that matches certain document\n",
    "            and term (from inverted index)\n",
    "    '''\n",
    "    # len(document.position_text) - number of term appearances in the text\n",
    "    # len(document.position_title) - number of term appearances in the title\n",
    "    # w - weight for word in title\n",
    "    w = 5\n",
    "    return np.log(1 + (len(document.position_text) + w*len(document.position_title))\n",
    "                  / document.doc_len)\n",
    "\n",
    "\n",
    "def normalized_tf_idf_docs(inverted_index, docs, idf):\n",
    "    '''\n",
    "    The function creates dictionary for every document \n",
    "    with normalized tf_idf for terms that appear in this document.\n",
    "    \n",
    "    Input: dictionary with idf for every term; DataFrame with document;\n",
    "            inverted index.\n",
    "    Output: {\n",
    "            docID1: {term1: tf_idf_docID1_term1, term2: tf_idf_docID1_term2, ...},\n",
    "            docID2: {term1: tf_idf_docID2_term1, term3: tf_idf_docID2_term3, ...}, \n",
    "            ...,\n",
    "            docIDn: {...}\n",
    "            }\n",
    "    '''\n",
    "    tf_idf = {int(docID): {} for docID in docs.id.unique()}    \n",
    "    \n",
    "    for term in inverted_index.keys():\n",
    "        for document in inverted_index[term]:\n",
    "            tf_idf[document.id][term] = tf(document) * idf[term]\n",
    "            \n",
    "    # normalization:\n",
    "    for docID in tf_idf.keys():\n",
    "        norm = sum([i**2 for i in tf_idf[docID].values()])**.5\n",
    "        tf_idf[docID] = {i[0]: i[1] / norm for i in tf_idf[docID].items()}\n",
    "        \n",
    "    return tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersect_all(terms, inverted_index):\n",
    "    '''\n",
    "    terms - list of terms for wich we want to intersect set of documents\n",
    "    inverted_index - created inverted index\n",
    "    '''\n",
    "            \n",
    "    if terms == []:\n",
    "        return set()\n",
    "    \n",
    "    ans = set()\n",
    "    for term in terms:            \n",
    "        p = inverted_index.get(term)\n",
    "        posting_list = set()\n",
    "        for d in p:\n",
    "            posting_list.add(d.id)\n",
    "        if len(ans) == 0:\n",
    "            ans = posting_list\n",
    "        else:\n",
    "            ans = ans & posting_list\n",
    "    \n",
    "    # if posting lists for terms don't intersect\n",
    "    if ans == set():\n",
    "        # find doc frequency because we want to find the rarest (more informational) term \n",
    "        # df -> {doc_freq: term}\n",
    "        df = {len(inverted_index.get(term)): term for term in terms}\n",
    "        term_with_min_df = df[min(list(df.keys()))]\n",
    "        p = inverted_index.get(term_with_min_df)\n",
    "        for d in p:\n",
    "            ans.add(d.id)\n",
    "    \n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Document:\n",
    "    def __init__(self, id, count, term, text_tokens, title_tokens):\n",
    "        self.id = id\n",
    "        self.count = count\n",
    "        self.position_text = self.position_in_doc(text_tokens, term)\n",
    "        self.position_title = self.position_in_title(title_tokens, term)\n",
    "        self.title_flag = any(term in sent for sent in title_tokens)\n",
    "        self.position_sentence = self.position_in_sentence(text_tokens, term)\n",
    "        self.doc_len = self.get_doc_len(text_tokens, title_tokens)\n",
    "\n",
    "    def get_doc_len(self, text_tokens, title_tokens):\n",
    "        length = 0\n",
    "        \n",
    "        for sent in text_tokens:\n",
    "            length += len(sent)\n",
    "            \n",
    "        for sent in title_tokens:\n",
    "            length += len(sent)\n",
    "            \n",
    "        return length\n",
    "        \n",
    "    def position_in_doc(self, text_tokens, term):\n",
    "        flat_list_of_text_tokens = [item for sublist in text_tokens for item in sublist]\n",
    "        return [i for i, x in enumerate(flat_list_of_text_tokens) if x == term]\n",
    "\n",
    "    def position_in_title(self, title_tokens, term):\n",
    "        flat_list_of_title_tokens = [item for sublist in title_tokens for item in sublist]\n",
    "        return [i for i, x in enumerate(flat_list_of_title_tokens) if x == term]\n",
    "\n",
    "    def position_in_sentence(self, text_tokens, term):\n",
    "        return [i for i, x in enumerate(text_tokens) if term in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index = dict()\n",
    "read_files = set()\n",
    "docs = pd.DataFrame()\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Flask(__name__)\n",
    "\n",
    "@app.route(\"/reverseindex\", methods=[\"POST\"])\n",
    "def reverseindex():\n",
    "    global docs\n",
    "    global inverted_index\n",
    "\n",
    "    json_data = request.json\n",
    "    words = json_data['data']\n",
    "    print(words)\n",
    "    # create flat list from list of lists\n",
    "    words = [item for sublist in words for item in sublist]\n",
    "\n",
    "    # reject words that are not in the inverted index\n",
    "    words = [term for term in words if inverted_index.get(term) is not None]\n",
    "    \n",
    "    # intersect lists of documents for all processed words in query\n",
    "    documents = list(intersect_all(words, inverted_index))\n",
    "    \n",
    "    # ranking if len(documents) is more than one word\n",
    "    print(documents)\n",
    "    if len(documents) > 1:\n",
    "        response_ranked = requests.post('http://127.0.0.1:13541/rank',\n",
    "                                        json={'documents': documents,\n",
    "                                              'words': words})\n",
    "        parsed_ranked = json.loads(response_ranked.text)\n",
    "        documents = parsed_ranked['ranked']\n",
    "        print(parsed_ranked)\n",
    "    \n",
    "    # if maximal numbers of documents given, then select only the desired amount\n",
    "    if json_data.get('max_docs'):\n",
    "        documents = documents[:json_data.get('max_docs')]        \n",
    "    \n",
    "    # need this because type(docs['id']) is string in this dataFrame\n",
    "    documents = [str(x) for x in documents]\n",
    "            \n",
    "    # index of sentences(first/second/etc) in which there are words from query for every doc\n",
    "    pos = {key: [] for key in documents}\n",
    "    for term in words:        \n",
    "        p = inverted_index.get(term)\n",
    "        for docID in documents:\n",
    "            for d in p:\n",
    "                if d.id == int(docID):\n",
    "                    pos[docID] += d.position_sentence\n",
    "                        \n",
    "  \n",
    "    # get text of found documentss\n",
    "    ranked_documents = []\n",
    "    for document in documents:\n",
    "        ranked_documents += docs.loc[docs['id'] == document,\n",
    "                                     ['id', 'title', 'text', 'url']].to_dict('records')\n",
    "      \n",
    "        \n",
    "    print(ranked_documents)\n",
    "    return json.dumps({\"status\":\"ok\", \"got_data\":json_data['data'], \n",
    "                       \"processed_data\": ranked_documents, \"position\": pos})\n",
    "\n",
    "\n",
    "@app.route(\"/reverseindex/add\", methods=['POST'])\n",
    "def add():\n",
    "    global docs\n",
    "    global inverted_index\n",
    "    \n",
    "    path = config['Data']['Path']\n",
    "    files = set([f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))])\n",
    "    docs_new = pd.DataFrame()\n",
    "\n",
    "    for file_name in files:\n",
    "        if file_name not in read_files and '-01-' in file_name:\n",
    "            read_files.add(file_name)\n",
    "    \n",
    "    docs_new = read_docs(path, read_files)\n",
    "    \n",
    "    docs_new = docs_new.iloc[:100, :]\n",
    "    for index, row in docs_new.iterrows():\n",
    "        doc = dict()\n",
    "        doc['docID'] = int(row['id'])\n",
    "        \n",
    "        response_analyze = requests.post('http://127.0.0.1:13533/analyze',\n",
    "                                         json={'data' : row['text']})\n",
    "        doc['text_searchable'] = json.loads(response_analyze.text)['words']\n",
    "        \n",
    "        response_analyze = requests.post('http://127.0.0.1:13533/analyze',\n",
    "                                         json={'data' : row['title']})\n",
    "        doc['title_searchable'] = json.loads(response_analyze.text)['words']\n",
    "        \n",
    "        tokens_text = doc['text_searchable']\n",
    "        tokens_title = doc['title_searchable']\n",
    "\n",
    "        tokens_text_flat = [item for sublist in tokens_text for item in sublist]\n",
    "        tokens_title_flat = [item for sublist in tokens_title for item in sublist]\n",
    "                \n",
    "        number_of_occurrences = Counter(tokens_text_flat + tokens_title_flat)         \n",
    "        for term in set(tokens_text_flat + tokens_title_flat):\n",
    "            if term not in inverted_index:            \n",
    "                inverted_index[term] = [Document(doc['docID'],\n",
    "                                                  number_of_occurrences[term],\n",
    "                                                  term,\n",
    "                                                  tokens_text,\n",
    "                                                  tokens_title)] \n",
    "            else:\n",
    "                inverted_index[term].append(Document(doc['docID'],\n",
    "                                                     number_of_occurrences[term],\n",
    "                                                     term,\n",
    "                                                     tokens_text,\n",
    "                                                     tokens_title))\n",
    "                \n",
    "            \n",
    "    # refresh idf and sent into service for ranking  \n",
    "    docs = docs.append(docs_new, ignore_index=True)\n",
    "    docs.drop_duplicates(subset='id_job', keep='first', inplace=True)\n",
    "    \n",
    "    docs.to_csv('documents.csv', sep='\\t')\n",
    "      \n",
    "    #create idf and normilized tf_idf from saved inverted_index\n",
    "    dict_idf = idf(inverted_index)\n",
    "    dict_tf_idf = normalized_tf_idf_docs(inverted_index, docs, dict_idf)    \n",
    "    requests.post('http://127.0.0.1:13541/rank/idf', json={'idf' : dict_idf,\n",
    "                                                           'tf_idf': dict_tf_idf})\n",
    "\n",
    "    return json.dumps({\"status\":\"ok\"}, ensure_ascii=False)\n",
    "\n",
    "        \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(host='0.0.0.0', port=13538)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
