{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"use this function for creating inverted index\"\n",
    "start = time.time()\n",
    "inverted_index = create_inverted_index()\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"create idf from saved inverted_index\"\n",
    "requests.post('http://127.0.0.1:13541/rank/idf', json={'data' : idf(inverted_index)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"save inverted index for using at home\"\n",
    "with open('reverse_index.pickle', 'wb') as handle:\n",
    "    pickle.dump(inverted_index, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"you can use saved inverted index at home\"\n",
    "with open('reverse_index.pickle', 'rb') as file:\n",
    "    inverted_index = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_text = []\n",
    "for root, dirs, files in os.walk(\"../../Data/by\"):  \n",
    "    for filename in files:\n",
    "        if 'text' in filename and '-01-' in filename:\n",
    "            with gzip.open('../../Data/by/' + filename, 'rb') as f:\n",
    "                for line in f:\n",
    "                    try: \n",
    "                        docs_text.append(json.loads(line))\n",
    "                    except JSONDecodeError:\n",
    "                        print(\"Can't read file \" + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_docs = []\n",
    "for root, dirs, files in os.walk(\"../../Data/by\"):  \n",
    "    for filename in files:\n",
    "        if 'text' not in filename and '-01-' in filename:\n",
    "            with gzip.open('../../Data/by/' + filename, 'rb') as f:\n",
    "                for line in f:\n",
    "                    try: \n",
    "                        meta_docs.append(json.loads(line))\n",
    "                    except JSONDecodeError:\n",
    "                        print(\"Can't read file \" + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_docs = pd.DataFrame(meta_docs)\n",
    "meta_docs.drop_duplicates(subset='id', keep='first', inplace=True)\n",
    "docs_text = pd.DataFrame(docs_text)\n",
    "docs_text.drop_duplicates(subset='id_job', keep='first', inplace=True)\n",
    "docs=pd.merge(meta_docs, docs_text, how='inner', left_on='id', right_on='id_job')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "import os\n",
    "import configparser\n",
    "import time\n",
    "from json import JSONDecodeError\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_index_slice(index_slice):\n",
    "    result = dict()\n",
    "    for term in index_slice:\n",
    "        for doc in index_slice[term]:\n",
    "            if term in result:\n",
    "                result[term].append(doc.__dict__)\n",
    "            else:\n",
    "                result[term] = [doc.__dict__]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_docs(path, files):\n",
    "    meta_docs = []\n",
    "    docs_text = []\n",
    "    for file_name in files:\n",
    "        if 'text' not in file_name:\n",
    "            with gzip.open(path + file_name, 'rb') as f:\n",
    "                for line in f:\n",
    "                    try: \n",
    "                        meta_docs.append(json.loads(line))\n",
    "                    except JSONDecodeError:\n",
    "                        print(\"Can't read file \" + filename)\n",
    "        \n",
    "        if 'text' in file_name:\n",
    "            with gzip.open(path + file_name, 'rb') as f:\n",
    "                for line in f:\n",
    "                    try: \n",
    "                        docs_text.append(json.loads(line))\n",
    "                    except JSONDecodeError:\n",
    "                        print(\"Can't read file \" + filename)\n",
    "                    \n",
    "    meta_docs = pd.DataFrame(meta_docs)\n",
    "    meta_docs.drop_duplicates(subset='id', keep='first', inplace=True)\n",
    "    docs_text = pd.DataFrame(docs_text)\n",
    "    docs_text.drop_duplicates(subset='id_job', keep='first', inplace=True)\n",
    "    docs=pd.merge(meta_docs, docs_text, how='inner', left_on='id', right_on='id_job')\n",
    "    \n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def position_in_text(lst, term):\n",
    "    '''\n",
    "    lst - list of tokens\n",
    "    term - term that you are looking for\n",
    "    Return all positions of term in the list of tokens(0 = first token) \n",
    "    '''\n",
    "    return [i for i, x in enumerate(lst) if x == term]\n",
    "\n",
    "def position_in_sentences(lst, term):\n",
    "    '''\n",
    "    lst - list of list, where each nested list - seperate sentences\n",
    "    term - term that you are looking for\n",
    "    Return all sequence numbers of sentences in which you can find term (0 = first sentence) \n",
    "    '''    \n",
    "    return [i for i, x in enumerate(lst) if term in x]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idf(inverted_index):\n",
    "    '''\n",
    "    inverted_index - fresh inverted index\n",
    "    return inverted document frequency for each term in inverted_index\n",
    "    '''\n",
    "    number_of_docs = docs.shape[0]\n",
    "    idf = {term: np.log(number_of_docs / len(inverted_index[term])) for term in list(inverted_index.keys())}\n",
    "    return idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersect_all(terms, inverted_index):\n",
    "    '''\n",
    "    terms - list of terms for wich we want to intersect set of documents\n",
    "    inverted_index - created inverted index\n",
    "    '''\n",
    "            \n",
    "    if terms == []:\n",
    "        # we will create random SERP above\n",
    "        return set()\n",
    "    \n",
    "    ans = set()\n",
    "    for term in terms:            \n",
    "        p = inverted_index.get(term)\n",
    "        posting_list = set()\n",
    "        for d in p:\n",
    "            posting_list.add(d.id)\n",
    "        if len(ans) == 0:\n",
    "            ans = posting_list\n",
    "        else:\n",
    "            ans = ans & posting_list\n",
    "    \n",
    "    # if posting lists for terms don't intersect\n",
    "    if ans == set():\n",
    "        # find doc frequency because we want to find the rarest (more informational) term \n",
    "        # df -> {doc_freq: term}\n",
    "        df = {len(inverted_index.get(term)): term for term in terms}\n",
    "        term_with_min_df = df[min(list(df.keys()))]\n",
    "        p = inverted_index.get(term_with_min_df)\n",
    "        for d in p:\n",
    "            ans.add(d.id)\n",
    "    \n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Document:\n",
    "    def __init__(self, id, count, term, text_tokens, title_tokens):\n",
    "        self.id = id\n",
    "        self.count = count\n",
    "        self.position_text = self.position_in_doc(text_tokens, term)\n",
    "        self.position_title = self.position_in_title(title_tokens, term)\n",
    "        self.title_flag = any(term in sent for sent in title_tokens)\n",
    "        self.position_sentence = self.position_in_sentence(text_tokens, term)\n",
    "        self.doc_len = self.get_doc_len(text_tokens, title_tokens)\n",
    "\n",
    "    def get_doc_len(self, text_tokens, title_tokens):\n",
    "        length = 0\n",
    "        \n",
    "        for sent in text_tokens:\n",
    "            length += len(sent)\n",
    "            \n",
    "        for sent in title_tokens:\n",
    "            length += len(sent)\n",
    "            \n",
    "        return length\n",
    "        \n",
    "    def position_in_doc(self, text_tokens, term):\n",
    "        flat_list_of_text_tokens = [item for sublist in text_tokens for item in sublist]\n",
    "        return [i for i, x in enumerate(flat_list_of_text_tokens) if x == term]\n",
    "\n",
    "    def position_in_title(self, title_tokens, term):\n",
    "        flat_list_of_title_tokens = [item for sublist in title_tokens for item in sublist]\n",
    "        return [i for i, x in enumerate(flat_list_of_title_tokens) if x == term]\n",
    "\n",
    "    def position_in_sentence(self, text_tokens, term):\n",
    "        return [i for i, x in enumerate(text_tokens) if term in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index = dict()\n",
    "read_files = set()\n",
    "docs = pd.DataFrame()\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "from flask import Flask\n",
    "from flask import request\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route(\"/reverseindex\", methods=[\"POST\"])\n",
    "def reverseindex():\n",
    "    global docs\n",
    "    global inverted_index\n",
    "\n",
    "    json_data = request.json\n",
    "    words = json_data['data']\n",
    "\n",
    "    # create flat list from list of lists\n",
    "    words = [item for sublist in words for item in sublist]\n",
    "\n",
    "    # reject words that are not in the inverted index\n",
    "    words = [term for term in words if inverted_index.get(term) is not None]\n",
    "    \n",
    "    # intersect lists of documents for all processed words in query\n",
    "    documents = list(intersect_all(words, inverted_index))\n",
    "    \n",
    "    # ranking if len(query) is more than one word\n",
    "    index_slice = {}\n",
    "    if len(documents) > 1:\n",
    "        index_slice = {key: [] for key in words}\n",
    "        for term in words:\n",
    "            index_slice[term] = inverted_index[term]\n",
    "        \n",
    "        json_index_slice = decode_index_slice(index_slice)\n",
    "                        \n",
    "        response_ranked = requests.post('http://127.0.0.1:13541/rank', \n",
    "                                       json={'data' : json_index_slice, \n",
    "                                             'documents' : documents, \n",
    "                                             'words' : words})\n",
    "        parsed_ranked = json.loads(response_ranked.text)\n",
    "        documents = parsed_ranked['ranked']\n",
    "        print(parsed_ranked)\n",
    "\n",
    "    \n",
    "    # if maximal numbers of documents given, then select only the desired amount\n",
    "    if json_data.get('max_docs'):\n",
    "        documents = documents[:json_data.get('max_docs')]        \n",
    "    \n",
    "    # need this because type(docs['id']) is string in this dataFrame\n",
    "    documents = [str(x) for x in documents]\n",
    "\n",
    "    # index of sentences(first/second/etc) in which there are words from query for every doc\n",
    "    pos = {key: [] for key in documents}\n",
    "    for term in words:        \n",
    "        p = inverted_index.get(term)\n",
    "        for docID in documents:\n",
    "            for d in p:\n",
    "                if d.id == int(docID):\n",
    "                    pos[docID] += d.position_sentence\n",
    "                        \n",
    "  \n",
    "    #get text of found documentss\n",
    "    ranked_documents = []\n",
    "    for document in documents:\n",
    "        ranked_documents += docs.loc[docs['id']==document, ['id', 'title', 'text', 'url']].to_dict('records')\n",
    "      \n",
    "        \n",
    "    return json.dumps({\"status\":\"ok\", \"got_data\":json_data['data'], \n",
    "                       \"processed_data\": ranked_documents, \"position\": pos})\n",
    "\n",
    "\n",
    "@app.route(\"/reverseindex/add\", methods=['POST'])\n",
    "def add():\n",
    "    global docs\n",
    "    global inverted_index\n",
    "    \n",
    "    path = config['Data']['Path']\n",
    "    files = set([f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))])\n",
    "    docs_new = pd.DataFrame()\n",
    "\n",
    "    for file_name in files:\n",
    "        if file_name not in read_files and '-01-' in file_name:\n",
    "            read_files.add(file_name)\n",
    "    \n",
    "    docs_new = read_docs(path, read_files)\n",
    "    \n",
    "    docs_new = docs_new.iloc[:100, :]\n",
    "    for index, row in docs_new.iterrows():\n",
    "        doc = dict()\n",
    "        doc['docID'] = int(row['id'])\n",
    "        response_analyze = requests.post('http://127.0.0.1:13533/analyze',\n",
    "                                         json={'data' : row['text']})\n",
    "        doc['text_searchable'] = json.loads(response_analyze.text)['words']\n",
    "        response_analyze = requests.post('http://127.0.0.1:13533/analyze',\n",
    "                                         json={'data' : row['title']})\n",
    "        doc['title_searchable'] = json.loads(response_analyze.text)['words']\n",
    "        \n",
    "        tokens_text = doc['text_searchable']\n",
    "        tokens_title = doc['title_searchable']\n",
    "\n",
    "        tokens_text_flat = [item for sublist in tokens_text for item in sublist]\n",
    "        tokens_title_flat = [item for sublist in tokens_title for item in sublist]\n",
    "        \n",
    "        number_of_occurrences = Counter(tokens_text_flat+tokens_title_flat)\n",
    "        for term in set(tokens_text_flat+tokens_title_flat):\n",
    "            if term not in inverted_index:          \n",
    "                inverted_index[term] = [ Document(doc['docID'], number_of_occurrences[term],\n",
    "                                                  term,\n",
    "                                                  tokens_text,\n",
    "                                                  tokens_title)] \n",
    "            else:\n",
    "                inverted_index[term].append(Document(doc['docID'], number_of_occurrences[term],\n",
    "                                                  term,\n",
    "                                                  tokens_text,\n",
    "                                                  tokens_title))\n",
    "                \n",
    "            \n",
    "    # refresh idf and sent into service for ranking  \n",
    "    docs = docs.append(docs_new, ignore_index=True)\n",
    "    docs.drop_duplicates(subset='id_job', keep='first', inplace=True)\n",
    "    \n",
    "    docs.to_csv('documents.csv', sep='\\t')\n",
    "    \n",
    "    requests.post('http://127.0.0.1:13541/rank/idf', json={'data' : idf(inverted_index)})\n",
    "\n",
    "    return json.dumps({\"status\":\"ok\"}, ensure_ascii=False)\n",
    "\n",
    "        \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(host='0.0.0.0', port=13538)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
