{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"It's forward index for searching text and creating snippet\"\n",
    "import pandas as pd\n",
    "docs = pd.read_csv('Data/eval_texts.csv', sep='\\t')\n",
    "docs.drop_duplicates(subset='id', keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"use this function for creating inverted index\"\n",
    "inverted_index = create_inverted_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"you can use saved inverted index at home\"\n",
    "import pickle\n",
    "with open('reverse_index.pickle', 'rb') as file:\n",
    "    inverted_index = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"save inverted index for using at home\"\n",
    "import pickle\n",
    "with open('reverse_index.pickle', 'wb') as handle:\n",
    "    pickle.dump(inverted_index, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from flask import Flask\n",
    "from flask import request\n",
    "import json\n",
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route(\"/reverseindex\", methods=[\"POST\"])\n",
    "def reverseindex():\n",
    "    json_data = request.json\n",
    "    words = json_data['data']\n",
    "    \n",
    "    #intersect lists of documents for all processed words in query\n",
    "    documents = list(intersect_all(words, inverted_index))\n",
    "    \n",
    "    #if maximal numbers of documents given, then select only the desired amount\n",
    "    if json_data.get('max_docs'):\n",
    "        documents = documents[:json_data.get('max_docs')]\n",
    "    \n",
    "    #get text of found documentss\n",
    "    documents = docs[docs['id'].apply(lambda x : x in documents)].loc[:, ['id', 'text']].to_dict('records')\n",
    "    \n",
    "    return json.dumps({\"status\":\"ok\", \"got_data\":json_data['data'], \"processed_data\": documents})\n",
    "\n",
    "\n",
    "'''@app.route(\"/reverseindex/add\", methods=['POST'])\n",
    "def add():\n",
    "    TODO\n",
    "    Add doc to reverse index\n",
    "    Imagine we have new_texts.csv\n",
    "    We have to create new service with:    \n",
    "    docs = pd.read_csv('../../Data/new_texts.csv', sep='\\t')\n",
    "    docs.drop_duplicates(subset='id', keep='first', inplace=True)\n",
    "    for i in range(docs.shape[0]):\n",
    "        doc = dict()\n",
    "        doc['docID'] = int(docs.loc[i, 'id'])\n",
    "        response_analyze_title = requests.post('http://127.0.0.1:13533/analyze', json={'data' : docs.loc[i, 'text_title']})\n",
    "        doc['title_searchable'] = json.loads(response_analyze_title.text)['words']\n",
    "        response_analyze_text = requests.post('http://127.0.0.1:13533/analyze', json={'data' : docs.loc[i, 'text']})\n",
    "        doc['text_searchable'] = json.loads(response_analyze_text.text)['words']\n",
    "        response_add_index = requests.post('http://127.0.0.1:13538/reverseindex/add', json={'data' : doc})\n",
    "        \n",
    "        Also we need to join 'eval_texts.csv' with 'new_texts.csv. Where and how?\n",
    "'''    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(host='0.0.0.0', port=13538)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "def position_in_text(lst, term):\n",
    "    '''\n",
    "    lst - list of tokens\n",
    "    term - term that you are looking for\n",
    "    Return all positions of term in the list of tokens(0 = first token) \n",
    "    '''\n",
    "    return [i for i, x in enumerate(lst) if x == term]\n",
    "\n",
    "def create_inverted_index():\n",
    "    '''\n",
    "    Return inverted index in list of dictionaries\n",
    "    word -> [{documentID, count of occurencies in document, positions in document, title_flag}, ...]\n",
    "    '''\n",
    "    inverted_index = dict()\n",
    "    docs = pd.read_csv('../../Data/eval_texts.csv', sep='\\t')\n",
    "    docs.drop_duplicates(subset='id', keep='first', inplace=True)\n",
    "    for i in range(docs.shape[0]):\n",
    "        doc = dict()\n",
    "        doc['docID'] = int(docs.loc[i, 'id'])\n",
    "        #doc['text'] = docs.loc[i, 'text_searchable']\n",
    "        response_analyze = requests.post('http://127.0.0.1:13533/analyze', json={'data' : docs.loc[i, 'text']})\n",
    "        doc['text_searchable'] = json.loads(response_analyze.text)['words']\n",
    "        \n",
<<<<<<< HEAD
=======
    "        '''\n",
    "        #Imagine we have \"text_title\" column:\n",
    "        response_analyze_title = requests.post('http://127.0.0.1:13533/analyze', json={'data' : docs.loc[i, 'text_title']})\n",
    "        doc['title_searchable'] = json.loads(response_analyze_title.text)['words']\n",
    "        response_analyze_text = requests.post('http://127.0.0.1:13533/analyze', json={'data' : docs.loc[i, 'text']})\n",
    "        doc['text_searchable'] = json.loads(response_analyze_text.text)['words']\n",
    "        '''\n",
    "        \n",
>>>>>>> parent of e7fb308... Comments added
    "        #tokens = doc['text'].split(' ')\n",
    "        tokens = doc['text_searchable']\n",
    "        number_of_occurrences = Counter(tokens)         \n",
    "        for term in set(tokens):\n",
    "          \n",
    "          if term not in inverted_index:\n",
    "            inverted_index[term] = [ {'docID': doc['docID'], 'count': number_of_occurrences[term], 'pos': position_in_text(tokens, term)}]\n",
    "          \n",
    "          else:\n",
    "            inverted_index[term].append({'docID': doc['docID'], 'count': number_of_occurrences[term], 'pos': position_in_text(tokens, term)})\n",
    "    return(inverted_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" NEW FEATURE\n",
    "def add_inverted_index(doc):\n",
    "    '''\n",
    "    doc - dictionary: {docID, 'text_searchable', 'title_searchable'}\n",
    "    Add new information in already existing inverted index\n",
    "    word -> [{documentID, count of occurencies in document, positions in document, title_flag}, ...]\n",
    "    '''    \n",
    "    tokens_title = doc['title_searchable']\n",
    "    tokens_text = doc['text_searchable']\n",
    "    number_of_occurrences = Counter(tokens_text)         \n",
    "    for term in set(tokens_title) | set(tokens_text):\n",
    "        title_flag = 0\n",
    "        \n",
    "        if term not in inverted_index:            \n",
    "            \n",
    "            if term in set(tokens_title):\n",
    "                title_flag = 1\n",
    "           \n",
    "            inverted_index[term] = [ {'docID': doc['docID'], 'count': number_of_occurrences[term], \\\n",
    "                                            'pos': position_in_text(tokens, term), 'title_flag': title_flag}]\n",
    "   \n",
    "        else:\n",
    "            '''\n",
    "            Check if document with docID already exist in posting list for given term\n",
    "            '''\n",
    "            p = inverted_index.get(term)\n",
    "            posting_list=set()\n",
    "            for d in p:\n",
    "                posting_list.add(d['docID'])\n",
    "                \n",
    "            if doc['docID'] not in posting_list:\n",
    "                \n",
    "                if term in set(tokens_title):\n",
    "                    title_flag = 1\n",
    "                                \n",
    "          \n",
    "                inverted_index[term].append({'docID': doc['docID'], 'count': number_of_occurrences[term],\\\n",
    "                                    'pos': position_in_text(tokens, term), 'title_flag': title_flag})\n",
    "                \n",
    "               \n",
    "    return(inverted_index)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
>>>>>>> parent of e7fb308... Comments added
   "outputs": [],
   "source": [
    "def intersect_all(terms, inverted_index):\n",
    "    '''\n",
    "    terms - list of terms for wich we want to intersect set of documents\n",
    "    inverted_index - created inverted index\n",
    "    '''\n",
    "    ans = set()\n",
    "    for term in terms:            \n",
    "        p = inverted_index.get(term)\n",
    "        posting_list=set()\n",
    "        if not p:\n",
    "            return set()\n",
    "        for d in p:\n",
    "            posting_list.add(d['docID'])\n",
    "        if len(ans) == 0:\n",
    "            ans = posting_list\n",
    "        else:\n",
    "            ans = ans & posting_list\n",
    "\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
