{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import os\n",
    "import requests\n",
    "import pickle\n",
    "import configparser\n",
    "\n",
    "from flask import Flask\n",
    "from flask import request\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpellChecker:\n",
    "    def __init__(self, model):\n",
    "        self.model = model    \n",
    "        self.WORDS = self.words()\n",
    "                \n",
    "    def words(self):\n",
    "        return set(self.model.keys())\n",
    "    \n",
    "    def P(self, word): \n",
    "        return self.model.get(word, 0)\n",
    "\n",
    "    def correction(self, word): \n",
    "        \"Most probable spelling correction for word.\"\n",
    "        return max(self.candidates(word), key=self.P)\n",
    "    \n",
    "    def correct(self, words):\n",
    "        for word in words:\n",
    "            if len(word) > 10 and word not in self.WORDS:\n",
    "                return self.segment(word)\n",
    "        return [self.correction(word) for word in words]\n",
    "\n",
    "    def candidates(self, word): \n",
    "        \"Generate possible spelling corrections for word.\"\n",
    "        return (self.known([word]) or self.known(self.edits1(word)) or \n",
    "                    self.known(self.edits2(word)) or [word])\n",
    "\n",
    "    def known(self, words): \n",
    "        \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "        return set(words) & self.WORDS\n",
    "\n",
    "\n",
    "    def edits1(self, word):\n",
    "        \"All edits that are one edit away from `word`.\"\n",
    "        #letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "        letters = 'абвгдежзийклмнопрстуфхцчшщъыьэюя'\n",
    "        splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "        deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "        transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "        replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "        inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "        self.edits_1 = list(set(deletes + transposes + replaces + inserts))\n",
    "        return set(deletes + transposes + replaces + inserts)\n",
    "      \n",
    "    def edit(self, word):\n",
    "        \"All edits that are one edit away from `word`.\"\n",
    "        #letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "        letters = 'абвгдежзийклмнопрстуфхцчшщъыьэюя'\n",
    "        splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "        deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "        transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "        replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "        inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "        return set(deletes + transposes + replaces + inserts)\n",
    "        \n",
    "\n",
    "    def edits2(self, word): \n",
    "        \"All edits that are two edits away from `word`.\"\n",
    "        answer = self.edit(list(self.edits_1)[0])\n",
    "        for w in list(self.edits_1)[1:]:\n",
    "            answer |= self.edit(w)\n",
    "        return answer   \n",
    "    \n",
    "    def Pwords(self, words):\n",
    "        \"Probability of words, assuming each word is independent of others.\"\n",
    "        return self.product(self.model.get(w,1e-9) for w in words)\n",
    "\n",
    "    def product(self, nums):\n",
    "        \"Multiply the numbers together.  (Like `sum`, but with multiplication.)\"\n",
    "        result = 1\n",
    "        for x in nums:\n",
    "            result *= x\n",
    "        return result\n",
    "    \n",
    "    def splits(self, text, start=0, L=20):\n",
    "        \"Return a list of all (first, rest) pairs; start <= len(first) <= L.\"\n",
    "        return [(text[:i], text[i:]) \n",
    "                for i in range(start, min(len(text), L)+1)]\n",
    "\n",
    "    def segment(self, text):\n",
    "        \"Return a list of words that is the most probable segmentation of text.\"\n",
    "        if not text: \n",
    "            return []\n",
    "        else:\n",
    "            candidates = ([first] + self.segment(rest) \n",
    "                          for (first, rest) in self.splits(text, 1))\n",
    "            return max(candidates, key=self.Pwords)        \n",
    "        \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model for spell checking\n",
    "'''\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini')\n",
    "\n",
    "path = config['Data']['Path']\n",
    "'''\n",
    "\n",
    "all_words = list()  \n",
    "for root, dirs, files in os.walk(\"../Data\"):  \n",
    "    for filename in files:\n",
    "        if 'list_of_words' in filename:\n",
    "            with open('../Data/' + filename, 'rb') as file:\n",
    "                try: \n",
    "                    all_words += pickle.load(file)\n",
    "                except JSONDecodeError:\n",
    "                    print(\"Can't read file \" + filename)\n",
    "    \n",
    "probability_dist = Counter(all_words)\n",
    "N = sum(probability_dist.values())\n",
    "for key in probability_dist:\n",
    "    probability_dist[key] /= N\n",
    "    \n",
    "sc = SpellChecker(probability_dist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Flask(__name__)\n",
    "\n",
    "@app.route(\"/spellchecker\", methods=[\"POST\"])\n",
    "def checking():\n",
    "    json_data = request.json\n",
    "    query = json_data['query']\n",
    "    \n",
    "    # tokenizing (create list of lists where each list is separate token)\n",
    "    response_tokenized = requests.post('http://127.0.0.1:13549/analysis/tokenize', \n",
    "                                       json={'data' : query})    \n",
    "    parsed_tokenized = json.loads(response_tokenized.text)    \n",
    "    \n",
    "    # words - list of tokens from query\n",
    "    words = [item for sublist in parsed_tokenized['processed_data'] for item in sublist]\n",
    "    # create list of correct words\n",
    "    correct_words = sc.correct(words)\n",
    "    \n",
    "    state = 0\n",
    "    correct_query = query\n",
    "    for i in zip(words, correct_words):\n",
    "        if i[0] != i[1]:\n",
    "            state = 1\n",
    "            correct_query = ' '.join(correct_words)\n",
    "    \n",
    "    print(state, correct_query)\n",
    "    \n",
    "    return json.dumps({\"status\":\"ok\", \"got_data\":json_data['query'], \n",
    "                       \"processed_data\": correct_query, \"state\": state})\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(host='0.0.0.0', port=13539)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
